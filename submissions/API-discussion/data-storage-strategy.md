# è³‡æ–™å­˜å„²ç­–ç•¥è¨­è¨ˆæ–‡ä»¶

> ** ä¿å®ˆç­–ç•¥èªªæ˜**
> - æœ¬æ–‡ä»¶æ¡ç”¨**é¸æ“‡æ€§å¿«å–**ç­–ç•¥ï¼šåªåœ¨æ¥µé«˜é »æ“ä½œå’Œå°ç”¨æˆ¶é«”é©—é—œéµçš„åœ°æ–¹è¨­è¨ˆ cache
> - æ²¿ç”¨èˆŠ NOJ å·²é©—è­‰çš„å¿«å–ç­–ç•¥ï¼ˆæäº¤åˆ—è¡¨ã€ç”¨æˆ¶é«˜åˆ†ã€æ¬Šé™æª¢æŸ¥ã€Tokenï¼‰
> - å¿…é ˆå¯¦ä½œ**å¸ƒéš†éæ¿¾å™¨**ï¼ˆé˜²æ­¢å¿«å–ç©¿é€ï¼‰å’Œ**åˆ†æ•£å¼é–**ï¼ˆé˜²æ­¢å¿«å–æ“Šç©¿ï¼‰
> - TTL æ™‚é–“åƒè€ƒèˆŠ NOJï¼ˆ30ç§’ - 10åˆ†é˜ï¼‰ï¼Œå„ªå…ˆä¿è­‰è³‡æ–™ä¸€è‡´æ€§
> - **é‡é»å„ªåŒ–**: æŸ¥çœ‹ ranking å’Œæäº¤è©³æƒ…çš„ç”¨æˆ¶é«”é©—

## æ¦‚è¿°

æœ¬æ–‡ä»¶åˆ†æåŸæœ‰ NOJ ç³»çµ±çš„å¿«å–ä½¿ç”¨æ¨¡å¼ï¼Œä¸¦å®šç¾©åœ¨æ–° Submissions ç³»çµ±ä¸­ï¼Œå“ªäº›è³‡æ–™æ‡‰è©²å­˜æ”¾åœ¨è³‡æ–™åº«(PostgreSQL)ï¼Œå“ªäº›è³‡æ–™æ‡‰è©²å­˜æ”¾åœ¨å¿«å–(Redis)ï¼Œä»¥åŠç›¸æ‡‰çš„å­˜å–ç­–ç•¥å’Œå¿«å–å¤±æ•ˆæ©Ÿåˆ¶ã€‚

## åŸæœ‰ NOJ ç³»çµ±å¿«å–åˆ†æ

### å¯¦éš›ä½¿ç”¨çš„å¿«å–ç­–ç•¥

åŸºæ–¼å°åŸæœ‰ç³»çµ±çš„åˆ†æï¼Œç™¼ç¾ä»¥ä¸‹å¿«å–ä½¿ç”¨æ¨¡å¼ï¼š

#### 1. æäº¤åˆ—è¡¨æŸ¥è©¢å¿«å–
```python
# ä½ç½®ï¼šmodel/submission.py get_submission_list()
cache_key = '_'.join(map(str, (
    'SUBMISSION_LIST_API',
    user, problem_id, username, status, 
    language_type, course, offset, count, before, after
)))

# å¿«å–å…§å®¹ï¼šæŸ¥è©¢çµæœ + ç¸½æ•¸
{
    'submissions': [...],
    'submission_count': 150
}

# å¿«å–æ™‚é–“ï¼š15ç§’ (éå¸¸çŸ­)
cache.set(cache_key, json.dumps(data), 15)
```

#### 2. ç”¨æˆ¶é«˜åˆ†å¿«å– 
```python
# ä½ç½®ï¼šmongo/problem/problem.py get_high_score()
cache_key = f'high_score_{problem_id}_{user_id}'

# å¿«å–å…§å®¹ï¼šç”¨æˆ¶åœ¨ç‰¹å®šé¡Œç›®çš„æœ€é«˜åˆ†
high_score = 87

# å¿«å–æ™‚é–“ï¼š600ç§’ (10åˆ†é˜)
cache.set(key, high_score, ex=600)
```

#### 3. æäº¤æ¬Šé™å¿«å–
```python
# ä½ç½®ï¼šmongo/submission.py own_permission()
cache_key = f'SUBMISSION_PERMISSION_{submission_id}_{user_id}_{problem_id}'

# å¿«å–å…§å®¹ï¼šæ¬Šé™ç­‰ç´š (æ•¸å­—)
permission_level = 3  # MANAGER, STUDENT, OTHER, etc.

# å¿«å–æ™‚é–“ï¼š60ç§’ (1åˆ†é˜)
cache.set(key, permission_level, 60)
```

#### 4. æäº¤ Token å¿«å– (å®‰å…¨ç”¨é€”)
```python
# ä½ç½®ï¼šmongo/submission.py assign_token/verify_token
cache_key = f'stoekn_{submission_id}'

# å¿«å–å…§å®¹ï¼šä¸€æ¬¡æ€§é©—è­‰ token
token = 'KoNoSandboxDa'

# ç‰¹é»ï¼šé©—è­‰å¾Œç«‹å³åˆªé™¤ (ä¸€æ¬¡æ€§ä½¿ç”¨)
cache.delete(key)  # ä½¿ç”¨å¾Œåˆªé™¤
```

### åŸç³»çµ±å¿«å–ç‰¹é»ç¸½çµ

1. **æ¥µçŸ­å¿«å–æ™‚é–“**ï¼šå¤šæ•¸å¿«å–åªæœ‰ 15-60 ç§’ï¼Œé¿å…è³‡æ–™ä¸ä¸€è‡´
2. **æŸ¥è©¢çµæœå°å‘**ï¼šä¸»è¦å¿«å–è³‡æ–™åº«æŸ¥è©¢çµæœï¼Œä¸å¿«å–åŸå§‹è³‡æ–™
3. **å®‰å…¨å„ªå…ˆ**ï¼šæ¬Šé™å’Œ token ç›¸é—œå¿«å–éƒ½æœ‰çŸ­éæœŸæ™‚é–“
4. **ç°¡å–®çš„å¤±æ•ˆç­–ç•¥**ï¼šä¸»è¦ä¾è³´ TTLï¼Œæ²’æœ‰è¤‡é›œçš„ä¸»å‹•å¤±æ•ˆ

## è³‡æ–™åº«å­˜å„² (PostgreSQL)

### æ°¸ä¹…æ€§è³‡æ–™ - å¿…é ˆå­˜è³‡æ–™åº«

#### 1. æ ¸å¿ƒæ¥­å‹™è³‡æ–™ (ä¿æŒèˆ‡åŸç³»çµ±ä¸€è‡´)
```sql
-- submissions è¡¨ï¼šæäº¤è¨˜éŒ„
- id, problem_id, user_id, language_type
- source_code, status, score, ip_address  
- created_at, updated_at, judged_at
- execution_time, memory_usage

-- submission_results è¡¨ï¼šæ¸¬è©¦çµæœè©³æƒ…  
- submission_id, task_id, case_id
- status, execution_time, memory_usage
- output_minio_path (æª”æ¡ˆå­˜å„²è·¯å¾‘)

-- user_problem_stats è¡¨ï¼šç”¨æˆ¶çµ±è¨ˆ
- user_id, problem_id, best_score
- attempt_count, solved_status, first_solved_at

-- custom_tests è¡¨ï¼šè‡ªå®šç¾©æ¸¬è©¦
- user_id, problem_id, language_type
- source_code, input_data, expected_output
- result, created_at

-- code_drafts è¡¨ï¼šç¨‹å¼ç¢¼è‰ç¨¿
- user_id, problem_id, language_type  
- source_code, title, last_modified

-- editorials è¡¨ï¼šé¡Œè§£
- problem_id, author_id, title, content
- difficulty_rating, is_official, created_at

-- editorial_likes è¡¨ï¼šé¡Œè§£é»è®š
- editorial_id, user_id, created_at
```
-- submissions è¡¨ï¼šæäº¤è¨˜éŒ„
- id, problem_id, user_id, language_type
- source_code, status, score, ip_address
- created_at, updated_at, judged_at

-- submission_results è¡¨ï¼šæ¸¬è©¦çµæœ
- submission_id, task_id, case_id
- status, execution_time, memory_usage
- input_data, expected_output, actual_output

-- user_problem_stats è¡¨ï¼šç”¨æˆ¶çµ±è¨ˆ
- user_id, problem_id, best_score
- attempt_count, solved_status, first_solved_at

-- custom_tests è¡¨ï¼šè‡ªå®šç¾©æ¸¬è©¦
- user_id, problem_id, language_type
- source_code, input_data, expected_output
- result, created_at

-- code_drafts è¡¨ï¼šç¨‹å¼ç¢¼è‰ç¨¿
- user_id, problem_id, language_type
- source_code, title, last_modified

-- editorials è¡¨ï¼šé¡Œè§£
- problem_id, author_id, title, content
- difficulty_rating, is_official, created_at

-- editorial_likes è¡¨ï¼šé¡Œè§£é»è®š
- editorial_id, user_id, created_at
```

#### 2. ç”¨æˆ¶èªè­‰èˆ‡æ¬Šé™è³‡æ–™
```sql
-- ç”¨æˆ¶åŸºæœ¬è³‡æ–™
- user_id, username, email, role
- last_login, is_active, created_at

-- æ¬Šé™èˆ‡è§’è‰²è³‡æ–™
- permissions, group_memberships
- course_enrollments, problem_access
```

#### 3. ç³»çµ±é…ç½®è³‡æ–™
```sql
-- ç³»çµ±è¨­å®š
- rate_limit_settings, sandbox_configurations
- judging_configurations, scoring_rules

-- å¯©è¨ˆæ—¥èªŒ
- user_actions, api_access_logs
- security_events, error_logs
```

**ç‚ºä»€éº¼è¦æ”¾è³‡æ–™åº«ï¼Ÿ**
- **è³‡æ–™ä¸€è‡´æ€§**ï¼šACID ç‰¹æ€§ä¿è­‰è³‡æ–™å®Œæ•´æ€§
- **æŒä¹…æ€§**ï¼šé‡è¦æ¥­å‹™è³‡æ–™ä¸èƒ½éºå¤±
- **è¤‡é›œæŸ¥è©¢**ï¼šæ”¯æ´ SQL è¤‡é›œæŸ¥è©¢å’Œèšåˆ
- **é—œè¯æ€§**ï¼šæ”¯æ´è¡¨æ ¼é–“çš„é—œè¯æŸ¥è©¢
- **å‚™ä»½æ¢å¾©**ï¼šå®Œæ•´çš„å‚™ä»½å’Œæ¢å¾©æ©Ÿåˆ¶

## å¿«å–å­˜å„² (Redis) - ä¿å®ˆç­–ç•¥å¯¦ä½œ

### 1. æ¥µé«˜é »æŸ¥è©¢å¿«å–

#### 1.1 æäº¤åˆ—è¡¨å¿«å–  å¿…é ˆå¯¦ä½œ
**API**: `GET /submission/`  
**åŸå› **: å­¸ç”Ÿæœƒåè¦†åˆ·æ–°æŸ¥çœ‹åˆ¤é¡Œçµæœï¼Œè¶…é«˜é »æŸ¥è©¢

```python
# å¿«å–éµæ ¼å¼
cache_key = f"SUBMISSION_LIST:{user_id}:{problem_id}:{status}:{language}:{offset}:{limit}"

# å¿«å–å…§å®¹
{
    "submissions": [
        {
            "id": "507f1f77-bcf8-6cd7-9943-9011",
            "problem_id": 42,
            "status": "accepted",
            "score": 100,
            "language_type": "python",
            "created_at": "2025-11-02T10:30:00Z"
        }
    ],
    "total_count": 150,
    "cached_at": "2025-11-02T10:30:00Z"
}

# å¿«å–æ™‚é–“ï¼š30ç§’ï¼ˆåƒè€ƒèˆŠ NOJ çš„ 15ç§’ï¼Œç¨å¾®å»¶é•·ï¼‰
# å¤±æ•ˆæ¢ä»¶ï¼š
# 1. TTL è‡ªå‹•éæœŸï¼ˆä¸»è¦ç­–ç•¥ï¼‰
# 2. ç”¨æˆ¶æ–°æäº¤æ™‚ä¸»å‹•æ¸…é™¤ï¼ˆpattern: SUBMISSION_LIST:{user_id}:*ï¼‰
```

#### 1.2 ç”¨æˆ¶çµ±è¨ˆå¿«å–  å¿…é ˆå¯¦ä½œ

**API**: `GET /stats/user/{userId}`  
**åŸå› **: è¨ˆç®—å¯†é›†ï¼ˆèšåˆæŸ¥è©¢ï¼‰ï¼Œå€‹äººé é¢å’Œæ’è¡Œæ¦œæœƒé »ç¹æŸ¥è©¢

```python
# å¿«å–éµæ ¼å¼  
cache_key = f"USER_STATS:{user_id}"

# å¿«å–å…§å®¹
{
    "user_id": "123e4567...",
    "total_submissions": 234,
    "solved_problems": 45,
    "accepted_count": 89,
    "accuracy_rate": 0.76,
    "best_scores": {...},  # problem_id -> score
    "recent_activities": [...],
    "language_distribution": {...}
}

# å¿«å–æ™‚é–“ï¼š5åˆ†é˜ï¼ˆå…è¨±çµ±è¨ˆè³‡æ–™çŸ­æœŸå»¶é²ï¼‰
# å¤±æ•ˆæ¢ä»¶ï¼š
# 1. TTL è‡ªå‹•éæœŸï¼ˆä¸»è¦ç­–ç•¥ï¼‰
# 2. ç”¨æˆ¶æ–°æäº¤æ™‚ä¸»å‹•æ¸…é™¤
# å®‰å…¨æ©Ÿåˆ¶ï¼šä½¿ç”¨åˆ†æ•£å¼é–é˜²æ­¢é‡è¤‡è¨ˆç®—
```

#### 1.3 æäº¤è©³æƒ…å¿«å–  å¿…é ˆå¯¦ä½œ

**API**: `GET /submission/<submission>`  
**åŸå› **: æŸ¥çœ‹åˆ¤é¡Œçµæœå’Œç¨‹å¼ç¢¼ï¼Œç†±é–€ AC æäº¤è¢«é »ç¹æŸ¥çœ‹

```python
# å¿«å–éµæ ¼å¼
cache_key = f"SUBMISSION_DETAIL:{submission_id}"

# å¿«å–å…§å®¹
{
    "id": "507f1f77...",
    "user": {"id": "...", "username": "alice"},
    "problem_id": 42,
    "status": "accepted",
    "score": 100,
    "execution_time": 1234,
    "memory_usage": 5678,
    "language_type": "python",
    "created_at": "2025-11-02T10:30:00Z",
    "results": [...]  # åŒ…å«æ¸¬è©¦çµæœ
}

# å¿«å–æ™‚é–“ï¼š2åˆ†é˜
# å¿«å–æ¢ä»¶ï¼šåªå¿«å–å·²åˆ¤é¡Œå®Œæˆçš„æäº¤ï¼ˆstatus != 'pending'ï¼‰
# å¤±æ•ˆæ¢ä»¶ï¼šTTL è‡ªå‹•éæœŸ
# å®‰å…¨æ©Ÿåˆ¶ï¼šä½¿ç”¨å¸ƒéš†éæ¿¾å™¨é˜²æ­¢æŸ¥è©¢ä¸å­˜åœ¨çš„ submission_id
```

---

### 2. èˆŠ NOJ å·²é©—è­‰å¿«å–ï¼ˆæ²¿ç”¨ï¼‰

#### 2.1 ç”¨æˆ¶é¡Œç›®é«˜åˆ†å¿«å–  æ²¿ç”¨èˆŠ NOJ

**ä¾†æº**: èˆŠ NOJ `get_high_score()`  
**åŸå› **: è¨ˆç®—æˆæœ¬é«˜ï¼ŒèˆŠç³»çµ±å·²é©—è­‰æœ‰æ•ˆ

```python
# å¿«å–éµæ ¼å¼
cache_key = f"HIGH_SCORE:{problem_id}:{user_id}"

# å¿«å–å…§å®¹
{
    "user_id": "123e4567...",
    "problem_id": 42,
    "best_score": 87,
    "best_submission_id": "507f1f77...",
    "cached_at": "2025-11-02T10:30:00Z"
}

# å¿«å–æ™‚é–“ï¼š10åˆ†é˜ï¼ˆæ²¿ç”¨èˆŠ NOJ è¨­å®šï¼‰
# å¤±æ•ˆæ¢ä»¶ï¼šTTL è‡ªå‹•éæœŸ + ç”¨æˆ¶æ–°æäº¤è©²é¡Œæ™‚æ¸…é™¤
```

#### 2.2 æäº¤æ¬Šé™å¿«å–  æ²¿ç”¨èˆŠ NOJ

**ä¾†æº**: èˆŠ NOJ `own_permission()`  
**åŸå› **: è¤‡é›œçš„æ¬Šé™è¨ˆç®—ï¼ŒçŸ­æœŸå¿«å–æå‡æ•ˆèƒ½

```python
# å¿«å–éµæ ¼å¼
cache_key = f"SUBMISSION_PERMISSION:{submission_id}:{user_id}"

# å¿«å–å…§å®¹
{
    "can_view": true,
    "can_edit": false,
    "can_delete": false,
    "is_owner": true,
    "is_course_staff": false
}

# å¿«å–æ™‚é–“ï¼š1åˆ†é˜ï¼ˆæ²¿ç”¨èˆŠ NOJï¼Œå®‰å…¨å„ªå…ˆï¼‰
# å¤±æ•ˆæ¢ä»¶ï¼šTTL è‡ªå‹•éæœŸ
```

#### 2.3 é©—è­‰ Token å¿«å–  æ²¿ç”¨èˆŠ NOJ

**ä¾†æº**: èˆŠ NOJ `assign_token()` / `verify_token()`  
**åŸå› **: Sandbox èˆ‡å¾Œç«¯çš„å®‰å…¨é€šä¿¡æ©Ÿåˆ¶

```python
# å¿«å–éµæ ¼å¼
cache_key = f"TOKEN:{submission_id}"

# å¿«å–å…§å®¹
"random_token_string_for_sandbox"

# ç‰¹æ®Šè™•ç†ï¼š
# - ç„¡å›ºå®šéæœŸæ™‚é–“
# - é©—è­‰å¾Œç«‹å³åˆªé™¤ï¼ˆä¸€æ¬¡æ€§ä½¿ç”¨ï¼‰
# - ç”¨æ–¼ Sandbox å›å‚³åˆ¤é¡Œçµæœæ™‚çš„èº«ä»½é©—è­‰
```

---

### 3. ç”¨æˆ¶é«”é©—å„ªåŒ–å¿«å–

#### 3.1 æ’è¡Œæ¦œå¿«å–  é«”é©—å„ªåŒ–

**API**: `GET /ranking`  
**åŸå› **: ç”¨æˆ¶é«”é©—è¦æ±‚ï¼Œé¿å…æ¯æ¬¡éƒ½é‡æ–°è¨ˆç®—æ’å

```python
# å¿«å–éµæ ¼å¼
cache_key = f"RANKING:{scope}:{time_range}"
# ç¯„ä¾‹: RANKING:global:all_time
# ç¯„ä¾‹: RANKING:course:123:this_week

# å¿«å–å…§å®¹
{
    "rankings": [
        {
            "rank": 1,
            "user_id": "123e4567...",
            "username": "alice",
            "solved_count": 89,
            "total_score": 8900,
            "accepted_rate": 0.85
        }
    ],
    "total_users": 500,
    "last_updated": "2025-11-02T11:00:00Z"
}

# å¿«å–æ™‚é–“ï¼š5åˆ†é˜ï¼ˆå¹³è¡¡å³æ™‚æ€§èˆ‡æ•ˆèƒ½ï¼‰
# å¤±æ•ˆæ¢ä»¶ï¼š
# 1. TTL è‡ªå‹•éæœŸï¼ˆä¸»è¦ç­–ç•¥ï¼‰
# 2. å¯é¸ï¼šèƒŒæ™¯ä»»å‹™å®šæœŸæ›´æ–°
# å®‰å…¨æ©Ÿåˆ¶ï¼šä½¿ç”¨åˆ†æ•£å¼é–é˜²æ­¢é‡è¤‡è¨ˆç®—
```

---

### 4. æš«ä¸å¿«å–çš„è³‡æ–™ï¼ˆæœªä¾†æ“´å±•ï¼‰

ä»¥ä¸‹è³‡æ–™åœ¨ä¿å®ˆç­–ç•¥ä¸­**ä¸å¿«å–**ï¼Œå¾…ç³»çµ±è¦æ¨¡æ“´å¤§å¾Œå†è€ƒæ…®ï¼š

#### æœƒè©±è³‡æ–™
- ä½¿ç”¨ Django Session æ¡†æ¶è™•ç†
- ä¸é¡å¤–å¿«å–åˆ° Redis

#### API é™æµè³‡æ–™  
- ä½¿ç”¨ Django REST Framework çš„ throttling æ©Ÿåˆ¶
- æˆ–ä½¿ç”¨ Nginx å±¤é¢çš„é™æµ

#### è‡ªå®šç¾©æ¸¬è©¦çµæœ
- è‡¨æ™‚è³‡æ–™ï¼ŒåŸ·è¡Œå®Œå³å¯ä¸Ÿæ£„
- ä¸éœ€è¦å¿«å–

#### é¡Œè§£ç›¸é—œ
- è®€å–é »ç‡ä¸é«˜
- è³‡æ–™è®Šæ›´é »ç‡ä½
- ä¸æ˜¯æ ¸å¿ƒåŠŸèƒ½

#### æœå°‹çµæœ
- æŸ¥è©¢æ¢ä»¶å¤šæ¨£ï¼Œå¿«å–å‘½ä¸­ç‡ä½
- ç›´æ¥æŸ¥è©¢è³‡æ–™åº«æˆ–ä½¿ç”¨å°ˆç”¨æœå°‹å¼•æ“

## å¿«å–ç­–ç•¥èˆ‡å¤±æ•ˆæ©Ÿåˆ¶ï¼ˆä¿å®ˆç­–ç•¥å¯¦ä½œï¼‰

### 1. Cache-Aside æ¨¡å¼ï¼ˆä¸»è¦æ¡ç”¨ï¼‰

```python
from django.core.cache import cache

def get_user_stats(user_id):
    """ä½¿ç”¨ Cache-Aside æ¨¡å¼ç²å–ç”¨æˆ¶çµ±è¨ˆ"""
    # 1. å…ˆæŸ¥å¿«å–
    cache_key = f"USER_STATS:{user_id}"
    cached_data = cache.get(cache_key)
    
    if cached_data:
        return cached_data
    
    # 2. å¿«å– missï¼Œä½¿ç”¨åˆ†æ•£å¼é–é˜²æ­¢æ“Šç©¿
    lock_key = f"lock:{cache_key}"
    lock = acquire_distributed_lock(lock_key, timeout=5)
    
    try:
        # é›™é‡æª¢æŸ¥
        cached_data = cache.get(cache_key)
        if cached_data:
            return cached_data
        
        # 3. æŸ¥è³‡æ–™åº«ä¸¦è¨ˆç®—
        stats = calculate_user_stats_from_db(user_id)
        
        # 4. å¯«å…¥å¿«å–ï¼ˆ5åˆ†é˜ï¼‰
        cache.set(cache_key, stats, 300)
        
        return stats
    finally:
        release_distributed_lock(lock_key, lock)

def invalidate_user_stats(user_id):
    """ç”¨æˆ¶æœ‰æ–°æäº¤æ™‚æ¸…é™¤å¿«å–"""
    cache_key = f"USER_STATS:{user_id}"
    cache.delete(cache_key)
```

### 2. é˜²æ­¢å¿«å–ç©¿é€ï¼ˆå¸ƒéš†éæ¿¾å™¨ï¼‰ å¿…é ˆå¯¦ä½œ

```python
from pybloom_live import BloomFilter

class CachePenetrationProtection:
    def __init__(self):
        # åˆå§‹åŒ–å¸ƒéš†éæ¿¾å™¨ï¼ˆ100è¬å®¹é‡ï¼Œ0.1% èª¤åˆ¤ç‡ï¼‰
        self.bloom_filter = BloomFilter(capacity=1000000, error_rate=0.001)
        self._init_bloom_filter()
    
    def _init_bloom_filter(self):
        """å•Ÿå‹•æ™‚å°‡æ‰€æœ‰ submission_id åŠ å…¥å¸ƒéš†éæ¿¾å™¨"""
        submission_ids = Submission.objects.values_list('id', flat=True)
        for sid in submission_ids:
            self.bloom_filter.add(str(sid))
    
    def add_submission(self, submission_id):
        """æ–°æäº¤æ™‚åŠ å…¥å¸ƒéš†éæ¿¾å™¨"""
        self.bloom_filter.add(str(submission_id))
    
    def might_exist(self, submission_id):
        """æª¢æŸ¥æäº¤æ˜¯å¦å¯èƒ½å­˜åœ¨"""
        return str(submission_id) in self.bloom_filter
    
    def get_submission_safe(self, submission_id):
        """å®‰å…¨ç²å–æäº¤ï¼Œé˜²æ­¢ç©¿é€"""
        # 1. æª¢æŸ¥å¸ƒéš†éæ¿¾å™¨
        if not self.might_exist(submission_id):
            # ç¢ºå®šä¸å­˜åœ¨ï¼Œç›´æ¥è¿”å› 404
            raise Http404("Submission not found")
        
        # 2. æª¢æŸ¥å¿«å–
        cache_key = f"SUBMISSION_DETAIL:{submission_id}"
        cached_data = cache.get(cache_key)
        if cached_data:
            return cached_data
        
        # 3. æŸ¥è©¢è³‡æ–™åº«
        try:
            submission = Submission.objects.get(id=submission_id)
            submission_data = SubmissionSerializer(submission).data
            
            # 4. åªå¿«å–å·²åˆ¤é¡Œå®Œæˆçš„æäº¤
            if submission.status != 'pending':
                cache.set(cache_key, submission_data, 120)  # 2åˆ†é˜
            
            return submission_data
            
        except Submission.DoesNotExist:
            # å¸ƒéš†éæ¿¾å™¨èª¤åˆ¤ï¼Œå¿«å–ç©ºå€¼é˜²æ­¢é‡è¤‡æŸ¥è©¢
            cache.set(cache_key, None, 60)
            raise Http404("Submission not found")

# å…¨åŸŸå¯¦ä¾‹
penetration_protection = CachePenetrationProtection()
```

### 3. é˜²æ­¢å¿«å–æ“Šç©¿ï¼ˆåˆ†æ•£å¼é–ï¼‰ å¿…é ˆå¯¦ä½œ

```python
import redis
import uuid
import time
from django.core.cache import cache

class RedisDistributedLock:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def acquire(self, key, expire=10, timeout=5):
        """
        ç²å–åˆ†æ•£å¼é–ï¼ˆå¸¶è¶…æ™‚æ©Ÿåˆ¶ï¼‰
        
        Args:
            key: é–çš„éµ
            expire: é–çš„éæœŸæ™‚é–“ï¼ˆç§’ï¼‰
            timeout: ç²å–é–çš„è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
        
        Returns:
            identifier æˆ– None
        """
        identifier = str(uuid.uuid4())
        lock_key = f"lock:{key}"
        end_time = time.time() + timeout
        
        while time.time() < end_time:
            # å˜—è©¦ç²å–é–ï¼ˆNX: ä¸å­˜åœ¨æ‰è¨­å®šï¼ŒEX: éæœŸæ™‚é–“ï¼‰
            if self.redis.set(lock_key, identifier, nx=True, ex=expire):
                return identifier
            time.sleep(0.01)  # 10ms å¾Œé‡è©¦
        
        # è¶…æ™‚æœªç²å–åˆ°é–
        return None
    
    def release(self, key, identifier):
        """é‡‹æ”¾åˆ†æ•£å¼é–ï¼ˆLua è…³æœ¬ç¢ºä¿åŸå­æ€§ï¼‰"""
        lock_key = f"lock:{key}"
        lua_script = """
        if redis.call("get", KEYS[1]) == ARGV[1] then
            return redis.call("del", KEYS[1])
        else
            return 0
        end
        """
        return self.redis.eval(lua_script, 1, lock_key, identifier)

# ä½¿ç”¨ç¯„ä¾‹ï¼šé˜²æ­¢æ’è¡Œæ¦œè¢«é‡è¤‡è¨ˆç®—ï¼ˆå¸¶è¶…æ™‚é™ç´šï¼‰
def get_ranking_safe(scope, time_range):
    cache_key = f"RANKING:{scope}:{time_range}"
    
    # 1. æª¢æŸ¥å¿«å–
    cached_data = cache.get(cache_key)
    if cached_data:
        return cached_data
    
    # 2. ç²å–åˆ†æ•£å¼é–ï¼ˆæœ€å¤šç­‰å¾… 3 ç§’ï¼‰
    lock = distributed_lock.acquire(cache_key, expire=30, timeout=3)
    if not lock:
        # è¶…æ™‚é™ç´šç­–ç•¥ï¼šç›´æ¥æŸ¥è©¢è³‡æ–™åº«ï¼Œä¸ç­‰å¾…
        logger.warning(f"Failed to acquire lock for {cache_key}, falling back to direct query")
        return calculate_ranking(scope, time_range)
    
    try:
        # 3. é›™é‡æª¢æŸ¥
        cached_data = cache.get(cache_key)
        if cached_data:
            return cached_data
        
        # 4. è¨ˆç®—æ’è¡Œæ¦œï¼ˆè¨ˆç®—å¯†é›†ï¼‰
        ranking_data = calculate_ranking(scope, time_range)
        
        # 5. å¯«å…¥å¿«å–ï¼ˆ5åˆ†é˜ï¼‰
        cache.set(cache_key, ranking_data, 300)
        
        return ranking_data
    finally:
        # 6. é‡‹æ”¾é–
        distributed_lock.release(cache_key, lock)

# Redis å®¢æˆ¶ç«¯
from django_redis import get_redis_connection
redis_client = get_redis_connection("default")
distributed_lock = RedisDistributedLock(redis_client)
```

### 4. å¿«å–å¤±æ•ˆæ©Ÿåˆ¶ï¼ˆç°¡åŒ–ç‰ˆï¼‰

#### TTL è‡ªå‹•éæœŸï¼ˆä¸»è¦ç­–ç•¥ï¼‰

```python
# ä¿å®ˆç­–ç•¥çš„å¿«å–æ™‚é–“é…ç½®
CACHE_TIMEOUTS = {
    'submission_list': 30,        # 30ç§’ï¼ˆåƒè€ƒèˆŠ NOJï¼‰
    'user_stats': 300,            # 5åˆ†é˜
    'submission_detail': 120,     # 2åˆ†é˜
    'high_score': 600,            # 10åˆ†é˜ï¼ˆæ²¿ç”¨èˆŠ NOJï¼‰
    'permission': 60,             # 1åˆ†é˜ï¼ˆæ²¿ç”¨èˆŠ NOJï¼‰
    'ranking': 300,               # 5åˆ†é˜
}

# TTL ä¿è­‰æœ€çµ‚ä¸€è‡´æ€§ï¼š
# - å³ä½¿ signals å¤±æ•ˆæˆ–å»¶é²ï¼Œå¿«å–æœ€å¤šåœ¨ TTL æ™‚é–“å¾Œæœƒè‡ªå‹•éæœŸ
# - å„ªå…ˆä½¿ç”¨ signals ä¸»å‹•æ¸…é™¤ï¼ŒTTL ä½œç‚ºå…œåº•æ©Ÿåˆ¶
```

#### åŸºæ–¼äº‹ä»¶çš„å¤±æ•ˆï¼ˆæœ€å°åŒ–ï¼‰

```python
# Django signals è§¸ç™¼å¿«å–å¤±æ•ˆ
from django.db.models.signals import post_save
from django.dispatch import receiver

@receiver(post_save, sender=Submission)
def invalidate_submission_caches(sender, instance, created, **kwargs):
    """æäº¤å»ºç«‹å¾Œæ¸…é™¤ç›¸é—œå¿«å–"""
    if not created:
        return
    
    user_id = instance.user.id
    problem_id = instance.problem_id
    
    # 1. æ¸…é™¤ç”¨æˆ¶æäº¤åˆ—è¡¨å¿«å–ï¼ˆæ¨¡å¼åŒ¹é…ï¼‰
    pattern = f"SUBMISSION_LIST:{user_id}:*"
    delete_cache_pattern(pattern)
    
    # 2. æ¸…é™¤ç”¨æˆ¶çµ±è¨ˆå¿«å–
    cache.delete(f"USER_STATS:{user_id}")
    
    # 3. æ¸…é™¤ç”¨æˆ¶é¡Œç›®é«˜åˆ†å¿«å–
    cache.delete(f"HIGH_SCORE:{problem_id}:{user_id}")
    
    # 4. æ¸…é™¤æ’è¡Œæ¦œå¿«å–ï¼ˆå»¶é²5ç§’ï¼Œé¿å…é »ç¹æ¸…é™¤ï¼‰
    from django_rq import enqueue
    enqueue(invalidate_ranking_cache, delay=5)
    
    # 5. å°‡æ–° submission_id åŠ å…¥å¸ƒéš†éæ¿¾å™¨
    penetration_protection.add_submission(instance.id)

def delete_cache_pattern(pattern):
    """åˆªé™¤ç¬¦åˆæ¨¡å¼çš„æ‰€æœ‰å¿«å–"""
    try:
        from django_redis import get_redis_connection
        conn = get_redis_connection("default")
        keys = conn.keys(pattern)
        if keys:
            conn.delete(*keys)
    except Exception as e:
        logger.error(f"Cache pattern delete failed: {e}")

def invalidate_ranking_cache():
    """æ¸…é™¤æ‰€æœ‰æ’è¡Œæ¦œå¿«å–"""
    pattern = "RANKING:*"
    delete_cache_pattern(pattern)

### Redis è¶…æ™‚èˆ‡é™ç´šè™•ç†ï¼ˆé˜²æ­¢é˜»å¡ï¼‰ å¿…é ˆå¯¦ä½œ

```python
from django.core.cache import cache
import logging

logger = logging.getLogger(__name__)

class CacheWithFallback:
    """å¸¶é™ç´šæ©Ÿåˆ¶çš„å¿«å–æ“ä½œ"""
    
    def __init__(self, timeout=0.5):
        """
        Args:
            timeout: Redis æ“ä½œè¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰ï¼Œé è¨­ 0.5 ç§’
        """
        self.timeout = timeout
    
    def get_safe(self, key, fetch_function=None):
        """
        å®‰å…¨ç²å–å¿«å–ï¼ŒRedis æ•…éšœæ™‚é™ç´šåˆ°è³‡æ–™åº«
        
        Args:
            key: å¿«å–éµ
            fetch_function: Redis å¤±æ•—æ™‚çš„é™ç´šå‡½æ•¸
        
        Returns:
            å¿«å–è³‡æ–™æˆ–è³‡æ–™åº«æŸ¥è©¢çµæœ
        """
        try:
            # å˜—è©¦å¾ Redis ç²å–ï¼ˆå¸¶è¶…æ™‚ï¼‰
            result = cache.get(key, default=None)
            if result is not None:
                return result
            
            # å¿«å– missï¼Œä½† Redis æ­£å¸¸
            if fetch_function:
                result = fetch_function()
                # å˜—è©¦å¯«å…¥å¿«å–
                try:
                    cache.set(key, result, 300)
                except Exception as e:
                    logger.warning(f"Cache set failed: {e}")
                return result
            
            return None
            
        except Exception as e:
            # Redis æ•…éšœï¼Œé™ç´šåˆ°è³‡æ–™åº«
            logger.error(f"Redis get failed for {key}: {e}")
            if fetch_function:
                return fetch_function()
            return None
    
    def set_safe(self, key, value, timeout=300):
        """
        å®‰å…¨å¯«å…¥å¿«å–ï¼Œå¤±æ•—ä¸é˜»å¡ä¸»æµç¨‹
        
        Args:
            key: å¿«å–éµ
            value: å¿«å–å€¼
            timeout: TTLï¼ˆç§’ï¼‰
        
        Returns:
            True/False è¡¨ç¤ºæ˜¯å¦æˆåŠŸ
        """
        try:
            cache.set(key, value, timeout)
            return True
        except Exception as e:
            # Redis æ•…éšœï¼Œè¨˜éŒ„æ—¥èªŒä½†ä¸æ‹‹å‡ºç•°å¸¸
            logger.error(f"Redis set failed for {key}: {e}")
            return False

# å…¨åŸŸå¯¦ä¾‹
cache_fallback = CacheWithFallback(timeout=0.5)

# ä½¿ç”¨ç¯„ä¾‹ï¼šç”¨æˆ¶çµ±è¨ˆæŸ¥è©¢
def get_user_stats_with_fallback(user_id):
    """ç²å–ç”¨æˆ¶çµ±è¨ˆï¼ŒRedis æ•…éšœæ™‚ç›´æ¥æŸ¥è©¢è³‡æ–™åº«"""
    cache_key = f"USER_STATS:{user_id}"
    
    return cache_fallback.get_safe(
        cache_key,
        fetch_function=lambda: calculate_user_stats_from_db(user_id)
    )

# ä½¿ç”¨ç¯„ä¾‹ï¼šæäº¤åˆ—è¡¨æŸ¥è©¢
def get_submission_list_safe(user_id, **filters):
    """ç²å–æäº¤åˆ—è¡¨ï¼ŒRedis æ•…éšœæ™‚é™ç´š"""
    cache_key = f"SUBMISSION_LIST:{user_id}:{hash(str(filters))}"
    
    def fetch_from_db():
        submissions = Submission.objects.filter(user_id=user_id, **filters)
        return list(submissions.values())
    
    return cache_fallback.get_safe(cache_key, fetch_from_db)
```
```

## æ•ˆèƒ½è€ƒé‡èˆ‡æœ€ä½³å¯¦è¸

### 1. å¿«å–å¤§å°æ§åˆ¶èˆ‡å…§å­˜ç®¡ç† å¿…é ˆå¯¦ä½œ

#### Redis è¨˜æ†¶é«”é…ç½®
```python
# redis.conf æˆ– Docker ç’°å¢ƒè®Šæ•¸é…ç½®
REDIS_CONFIG = {
    'maxmemory': '2gb',                    # æœ€å¤§è¨˜æ†¶é«”é™åˆ¶
    'maxmemory-policy': 'allkeys-lru',     # LRU æ·˜æ±°ç­–ç•¥
    'maxmemory-samples': 5,                # LRU æ¡æ¨£æ•¸é‡
}

# æ·˜æ±°ç­–ç•¥èªªæ˜ï¼š
# - allkeys-lru: å¾æ‰€æœ‰ key ä¸­æ·˜æ±°æœ€å°‘ä½¿ç”¨çš„ï¼ˆæ¨è–¦ï¼‰
# - volatile-lru: åªå¾è¨­å®šéæœŸæ™‚é–“çš„ key ä¸­æ·˜æ±°
# - allkeys-lfu: å¾æ‰€æœ‰ key ä¸­æ·˜æ±°è¨ªå•é »ç‡æœ€ä½çš„
# - volatile-ttl: å„ªå…ˆæ·˜æ±° TTL æœ€çŸ­çš„ key
# - noeviction: è¨˜æ†¶é«”æ»¿æ™‚æ‹’çµ•å¯«å…¥ï¼ˆä¸æ¨è–¦ï¼‰

# Docker Compose é…ç½®ç¯„ä¾‹
"""
services:
  redis:
    image: redis:7-alpine
    command: >
      redis-server
      --maxmemory 2gb
      --maxmemory-policy allkeys-lru
      --maxmemory-samples 5
    ports:
      - "6379:6379"
"""

#### è¨˜æ†¶é«”ç›£æ§èˆ‡è­¦å ± å¿…é ˆå¯¦ä½œ

```python
from django_redis import get_redis_connection
import logging

logger = logging.getLogger(__name__)

class RedisMemoryMonitor:
    """Redis è¨˜æ†¶é«”ç›£æ§"""
    
    def __init__(self, warning_threshold=0.8, critical_threshold=0.9):
        """
        Args:
            warning_threshold: è­¦å‘Šé–¾å€¼ï¼ˆ80%ï¼‰
            critical_threshold: åš´é‡é–¾å€¼ï¼ˆ90%ï¼‰
        """
        self.redis = get_redis_connection("default")
        self.warning_threshold = warning_threshold
        self.critical_threshold = critical_threshold
    
    def get_memory_info(self):
        """ç²å– Redis è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³"""
        try:
            info = self.redis.info('memory')
            used_memory = info['used_memory']
            max_memory = info.get('maxmemory', 0)
            
            if max_memory == 0:
                logger.warning("Redis maxmemory not set!")
                return None
            
            usage_ratio = used_memory / max_memory
            
            return {
                'used_memory_mb': used_memory / (1024 * 1024),
                'max_memory_mb': max_memory / (1024 * 1024),
                'usage_ratio': usage_ratio,
                'status': self._get_status(usage_ratio)
            }
        except Exception as e:
            logger.error(f"Failed to get Redis memory info: {e}")
            return None
    
    def _get_status(self, usage_ratio):
        """åˆ¤æ–·è¨˜æ†¶é«”ä½¿ç”¨ç‹€æ…‹"""
        if usage_ratio >= self.critical_threshold:
            return 'CRITICAL'
        elif usage_ratio >= self.warning_threshold:
            return 'WARNING'
        else:
            return 'OK'
    
    def check_and_alert(self):
        """æª¢æŸ¥ä¸¦ç™¼é€è­¦å ±"""
        info = self.get_memory_info()
        if not info:
            return
        
        status = info['status']
        usage = info['usage_ratio']
        
        if status == 'CRITICAL':
            logger.critical(
                f"Redis memory CRITICAL: {usage:.1%} used "
                f"({info['used_memory_mb']:.1f}MB / {info['max_memory_mb']:.1f}MB)"
            )
            # TODO: ç™¼é€ç·Šæ€¥é€šçŸ¥ï¼ˆEmail/Slackï¼‰
            
        elif status == 'WARNING':
            logger.warning(
                f"Redis memory WARNING: {usage:.1%} used "
                f"({info['used_memory_mb']:.1f}MB / {info['max_memory_mb']:.1f}MB)"
            )
        else:
            logger.info(f"Redis memory OK: {usage:.1%} used")
        
        return info

# å…¨åŸŸç›£æ§å¯¦ä¾‹
memory_monitor = RedisMemoryMonitor()

# Django Management Command: ç›£æ§ Redis è¨˜æ†¶é«”
# python manage.py monitor_redis_memory
from django.core.management.base import BaseCommand

class Command(BaseCommand):
    help = 'Monitor Redis memory usage'
    
    def handle(self, *args, **options):
        info = memory_monitor.check_and_alert()
        if info:
            self.stdout.write(
                f"Memory: {info['used_memory_mb']:.1f}MB / "
                f"{info['max_memory_mb']:.1f}MB ({info['usage_ratio']:.1%})"
            )
```

#### å¤§ç‰©ä»¶å£“ç¸®å­˜å„²ï¼ˆé¸ç”¨ï¼‰

```python
import gzip
import json

def cache_large_object(key, data, timeout):
    """å£“ç¸®å¤§å‹è³‡æ–™å¾Œå¿«å–ï¼ˆç¯€çœ 50-70% è¨˜æ†¶é«”ï¼‰"""
    compressed_data = gzip.compress(json.dumps(data).encode())
    cache.set(f"gz:{key}", compressed_data, timeout)

def get_large_object(key):
    """ç²å–å£“ç¸®å¿«å–è³‡æ–™"""
    compressed_data = cache.get(f"gz:{key}")
    if compressed_data:
        return json.loads(gzip.decompress(compressed_data).decode())
    return None

# ä½¿ç”¨ç¯„ä¾‹ï¼šæ’è¡Œæ¦œè³‡æ–™è¼ƒå¤§æ™‚
def cache_ranking_compressed(scope, ranking_data):
    cache_key = f"RANKING:{scope}"
    cache_large_object(cache_key, ranking_data, 300)
```

#### å¿«å–å¤§å°é™åˆ¶ï¼ˆä¿å®ˆç­–ç•¥ï¼‰

```python
# ä¿å®ˆç­–ç•¥çš„å¿«å–å¤§å°æ§åˆ¶
MAX_CACHE_SIZES = {
    'submission_list': 100,       # æœ€å¤š 100 ç­†æäº¤è¨˜éŒ„
    'ranking': 500,               # æœ€å¤š 500 åç”¨æˆ¶æ’å
    'user_stats': None,           # ç„¡é™åˆ¶ï¼ˆå–®å€‹ç‰©ä»¶å°ï¼‰
}

def cache_submission_list_limited(cache_key, submissions, timeout=30):
    """é™åˆ¶æäº¤åˆ—è¡¨å¿«å–å¤§å°"""
    # åªå¿«å–å‰ 100 ç­†
    limited_data = submissions[:MAX_CACHE_SIZES['submission_list']]
    cache.set(cache_key, limited_data, timeout)
```
```

### 2. ç›£æ§å’Œè­¦å ±ç³»çµ± å¿…é ˆå¯¦ä½œ

#### å¿«å–å‘½ä¸­ç‡ç›£æ§ï¼ˆç°¡åŒ–ç‰ˆï¼‰

```python
import logging
from collections import defaultdict

logger = logging.getLogger(__name__)

class CacheHitRateMonitor:
    """å¿«å–å‘½ä¸­ç‡ç›£æ§"""
    
    def __init__(self):
        self.stats = defaultdict(lambda: {'hits': 0, 'misses': 0})
    
    def record_hit(self, cache_type):
        """è¨˜éŒ„å¿«å–å‘½ä¸­"""
        self.stats[cache_type]['hits'] += 1
    
    def record_miss(self, cache_type):
        """è¨˜éŒ„å¿«å–æœªå‘½ä¸­"""
        self.stats[cache_type]['misses'] += 1
    
    def get_hit_rate(self, cache_type):
        """è¨ˆç®—å‘½ä¸­ç‡"""
        stats = self.stats[cache_type]
        total = stats['hits'] + stats['misses']
        if total == 0:
            return 0.0
        return stats['hits'] / total
    
    def report(self):
        """ç”Ÿæˆç›£æ§å ±å‘Š"""
        for cache_type, stats in self.stats.items():
            total = stats['hits'] + stats['misses']
            if total == 0:
                continue
            
            hit_rate = self.get_hit_rate(cache_type)
            status = 'âœ…' if hit_rate >= 0.7 else 'âš ï¸' if hit_rate >= 0.5 else 'ğŸ”´'
            
            logger.info(
                f"{status} Cache[{cache_type}]: "
                f"Hit Rate {hit_rate:.1%} ({stats['hits']}/{total})"
            )
            
            # ä½å‘½ä¸­ç‡è­¦å ±
            if hit_rate < 0.5 and total > 100:
                logger.warning(f"Low hit rate for {cache_type}: {hit_rate:.1%}")

# å…¨åŸŸç›£æ§å¯¦ä¾‹
hit_rate_monitor = CacheHitRateMonitor()

# ä½¿ç”¨ç¯„ä¾‹
def get_user_stats_monitored(user_id):
    cache_key = f"USER_STATS:{user_id}"
    cached_data = cache.get(cache_key)
    
    if cached_data:
        hit_rate_monitor.record_hit('user_stats')
        return cached_data
    
    hit_rate_monitor.record_miss('user_stats')
    stats = calculate_user_stats_from_db(user_id)
    cache.set(cache_key, stats, 300)
    return stats
```

#### æ‰¹é‡æ“ä½œæœ€ä½³åŒ–
```python
# æ‰¹é‡æŸ¥è©¢å¿«å–
def get_multiple_submissions(submission_ids):
    cache_keys = [f"submission:{sid}" for sid in submission_ids]
    cached_results = redis.mget(cache_keys)
    
    # æ‰¾å‡º cache miss çš„ IDs
    missing_ids = []
    results = {}
    
    for i, result in enumerate(cached_results):
        if result:
            results[submission_ids[i]] = json.loads(result)
        else:
            missing_ids.append(submission_ids[i])
    
    # åªæŸ¥è©¢ cache miss çš„è³‡æ–™
    if missing_ids:
        db_results = Submission.objects.filter(id__in=missing_ids)
        for submission in db_results:
            results[submission.id] = submission.to_dict()
            # å›å¯«å¿«å–
            redis.setex(f"submission:{submission.id}", 1800, 
                       json.dumps(submission.to_dict()))
    
    return results
```

### 2. ç›£æ§èˆ‡è­¦å ±

#### å¿«å–å‘½ä¸­ç‡ç›£æ§
```python
def cache_hit_rate_middleware(get_response):
    def middleware(request):
        # è¨˜éŒ„å¿«å–å­˜å–çµ±è¨ˆ
        cache_stats = {
            'hits': 0,
            'misses': 0,
            'operations': []
        }
        
        response = get_response(request)
        
        # è¨˜éŒ„åˆ°ç›£æ§ç³»çµ±
        hit_rate = cache_stats['hits'] / (cache_stats['hits'] + cache_stats['misses'])
        if hit_rate < 0.8:  # å‘½ä¸­ç‡ä½æ–¼ 80% æ™‚è­¦å ±
            logger.warning(f"Low cache hit rate: {hit_rate:.2%}")
        
        return response
    return middleware
```

#### è¨˜æ†¶é«”ä½¿ç”¨ç›£æ§
```python
def monitor_redis_memory():
    info = redis.info('memory')
    used_memory = info['used_memory']
    max_memory = info.get('maxmemory', 0)
    
    if max_memory > 0:
        usage_percent = (used_memory / max_memory) * 100
        if usage_percent > 90:
            logger.critical(f"Redis memory usage: {usage_percent:.1f}%")
        elif usage_percent > 80:
            logger.warning(f"Redis memory usage: {usage_percent:.1f}%")
```

## å®‰å…¨æ€§è€ƒé‡

### 1. å¿«å–è³‡æ–™å®‰å…¨
```python
# æ•æ„Ÿè³‡æ–™ä¸æ”¾å¿«å–ï¼Œæˆ–åŠ å¯†å­˜å„²
def cache_sensitive_data(key, data, timeout):
    # åŠ å¯†æ•æ„Ÿè³‡æ–™
    encrypted_data = encrypt(json.dumps(data))
    redis.setex(f"secure:{key}", timeout, encrypted_data)

def get_sensitive_data(key):
    encrypted_data = redis.get(f"secure:{key}")
    if encrypted_data:
        return json.loads(decrypt(encrypted_data))
    return None
```

### 2. å¿«å–éš”é›¢
```python
# ç”¨æˆ¶è³‡æ–™éš”é›¢
def get_user_cache_key(user_id, key_type, *args):
    # ç¢ºä¿ç”¨æˆ¶åªèƒ½å­˜å–è‡ªå·±çš„å¿«å–
    return f"user:{user_id}:{key_type}:{':'.join(map(str, args))}"

# æ¬Šé™æª¢æŸ¥
def get_cached_data_with_permission(user, cache_key):
    if not user.has_permission_for_cache(cache_key):
        raise PermissionError("Access denied")
    return redis.get(cache_key)
```

## ç¸½çµ

### ä¿å®ˆç­–ç•¥å¿«å–æ¸…å–®

| è³‡æ–™é¡å‹ | Cache Key | TTL | ä¾†æº | å®‰å…¨æ©Ÿåˆ¶ |
|---------|-----------|-----|------|---------|
| æäº¤åˆ—è¡¨ | `SUBMISSION_LIST:{user_id}:{problem_id}:{status}:{offset}:{limit}` | 30ç§’ | æ¥µé«˜é » | è¶…æ™‚é™ç´š |
| ç”¨æˆ¶çµ±è¨ˆ | `USER_STATS:{user_id}` | 5åˆ†é˜ | æ¥µé«˜é » | åˆ†æ•£å¼é– + è¶…æ™‚é™ç´š |
| æäº¤è©³æƒ… | `SUBMISSION_DETAIL:{submission_id}` | 2åˆ†é˜ | æ¥µé«˜é » | å¸ƒéš†éæ¿¾å™¨ + è¶…æ™‚é™ç´š |
| ç”¨æˆ¶é¡Œç›®é«˜åˆ† | `HIGH_SCORE:{problem_id}:{user_id}` | 10åˆ†é˜ | èˆŠ NOJ | è¶…æ™‚é™ç´š |
| æäº¤æ¬Šé™ | `SUBMISSION_PERMISSION:{submission_id}:{user_id}` | 1åˆ†é˜ | èˆŠ NOJ | è¶…æ™‚é™ç´š |
| é©—è­‰ Token | `TOKEN:{submission_id}` | ä¸€æ¬¡æ€§ | èˆŠ NOJ | ä½¿ç”¨å¾Œåˆªé™¤ |
| æ’è¡Œæ¦œ | `RANKING:{scope}:{time_range}` | 5åˆ†é˜ | é«”é©—å„ªåŒ– | åˆ†æ•£å¼é– + è¶…æ™‚é™ç´š |

### å¿…é ˆå¯¦ä½œçš„æ ¸å¿ƒæ©Ÿåˆ¶

#### 1. å¸ƒéš†éæ¿¾å™¨ï¼ˆé˜²æ­¢å¿«å–ç©¿é€ï¼‰
- é˜²æ­¢æŸ¥è©¢ä¸å­˜åœ¨çš„ `submission_id`
- 100è¬å®¹é‡ï¼Œ0.1% èª¤åˆ¤ç‡
- å•Ÿå‹•æ™‚è¼‰å…¥æ‰€æœ‰ ID
- æ–°æäº¤æ™‚å‹•æ…‹åŠ å…¥

#### 2. åˆ†æ•£å¼é–ï¼ˆé˜²æ­¢å¿«å–æ“Šç©¿ï¼‰
- é˜²æ­¢çµ±è¨ˆè³‡æ–™å’Œæ’è¡Œæ¦œçš„é‡è¤‡è¨ˆç®—
- ä½¿ç”¨ Redis SET NX EX
- Lua è…³æœ¬ç¢ºä¿åŸå­æ€§é‡‹æ”¾
- **å¸¶è¶…æ™‚æ©Ÿåˆ¶**ï¼šæœ€å¤šç­‰å¾… 3-5 ç§’

#### 3. è¶…æ™‚é™ç´šï¼ˆé˜²æ­¢ Redis é˜»å¡ï¼‰
- Redis æ“ä½œè¶…æ™‚ï¼ˆ0.5 ç§’ï¼‰è‡ªå‹•é™ç´š
- é™ç´šç­–ç•¥ï¼šç›´æ¥æŸ¥è©¢è³‡æ–™åº«
- å¿«å–å¯«å…¥å¤±æ•—ä¸é˜»å¡ä¸»æµç¨‹
- è¨˜éŒ„æ—¥èªŒä½†ä¸æ‹‹å‡ºç•°å¸¸

#### 4. å…§å­˜ç®¡ç†ï¼ˆé˜²æ­¢è¨˜æ†¶é«”çˆ†æ»¿ï¼‰
- Redis `maxmemory` é™åˆ¶ï¼š2GB
- æ·˜æ±°ç­–ç•¥ï¼š`allkeys-lru`
- è¨˜æ†¶é«”ç›£æ§ï¼š80% è­¦å‘Šï¼Œ90% åš´é‡
- å®šæœŸç›£æ§ä»»å‹™ï¼š`python manage.py monitor_redis_memory`

#### 5. å¿«å–ä¸€è‡´æ€§ï¼ˆé˜²æ­¢è³‡æ–™ä¸ä¸€è‡´ï¼‰
- Django signals ä¸»å‹•æ¸…é™¤å¿«å–
- TTL è‡ªå‹•éæœŸä½œç‚ºå…œåº•æ©Ÿåˆ¶
- æœ€å¤§ä¸ä¸€è‡´æ™‚é–“ï¼š5 åˆ†é˜ï¼ˆæ’è¡Œæ¦œï¼‰
- é—œéµè³‡æ–™ï¼ˆæ¬Šé™ï¼‰ï¼š1 åˆ†é˜ TTL

#### 6. ç›£æ§èˆ‡è­¦å ±ï¼ˆç™¼ç¾å•é¡Œï¼‰
- å¿«å–å‘½ä¸­ç‡ç›£æ§ï¼šç›®æ¨™ â‰¥ 70%
- Redis è¨˜æ†¶é«”ç›£æ§ï¼š80% è­¦å‘Š
- Redis é€£ç·šç‹€æ…‹æª¢æŸ¥
- Management Commandï¼š`python manage.py cache_stats`


### è³‡æ–™åº« vs å¿«å–æ±ºç­–æº–å‰‡

| ç‰¹æ€§ | è³‡æ–™åº« | å¿«å– |
|-----|--------|------|
| **è³‡æ–™é‡è¦æ€§** | æ ¸å¿ƒæ¥­å‹™è³‡æ–™ã€ä¸å¯éºå¤± | å¯é‡å»ºçš„æŸ¥è©¢çµæœ |
| **å­˜å–é »ç‡** | å„ç¨®é »ç‡ | æ¥µé«˜é »ï¼ˆæäº¤åˆ—è¡¨ã€çµ±è¨ˆï¼‰ |
| **è³‡æ–™å¤§å°** | ä»»æ„å¤§å° | ç›¸å°è¼ƒå° |
| **æŸ¥è©¢è¤‡é›œåº¦** | è¤‡é›œ SQL æŸ¥è©¢ | ç°¡å–® key-value æŸ¥è©¢ |
| **ä¸€è‡´æ€§è¦æ±‚** | å¼·ä¸€è‡´æ€§ | æœ€çµ‚ä¸€è‡´æ€§å¯æ¥å—ï¼ˆâ‰¤30ç§’ï¼‰ |
| **æŒä¹…æ€§è¦æ±‚** | æ°¸ä¹…ä¿å­˜ | è‡¨æ™‚å­˜å„² |

### å¯¦ä½œå„ªå…ˆç´š

#### ç¬¬ä¸€éšæ®µï¼ˆæ ¸å¿ƒåŠŸèƒ½ - å¿…é ˆå®Œæˆï¼‰
1. **Redis åŸºç¤é…ç½®**
   ```bash
   pip install django-redis pybloom-live redis
   ```
   - é…ç½® Django settings
   - è¨­å®š `maxmemory=2gb` å’Œ `maxmemory-policy=allkeys-lru`
   - Docker Compose é…ç½®

2. **å®‰å…¨æ©Ÿåˆ¶å¯¦ä½œ**
   - `CachePenetrationProtection`ï¼ˆå¸ƒéš†éæ¿¾å™¨ï¼‰
   - `RedisDistributedLock`ï¼ˆåˆ†æ•£å¼é–ï¼‰
   - `CacheWithFallback`ï¼ˆè¶…æ™‚é™ç´šï¼‰

3. **æ¥µé«˜é »å¿«å–**
   - æäº¤åˆ—è¡¨å¿«å–ï¼ˆ30ç§’ TTLï¼‰
   - ç”¨æˆ¶çµ±è¨ˆå¿«å–ï¼ˆ5åˆ†é˜ TTLï¼‰
   - Django signals æ¸…é™¤æ©Ÿåˆ¶

#### ç¬¬äºŒéšæ®µï¼ˆå„ªåŒ–é«”é©—ï¼‰
1. **ç”¨æˆ¶é«”é©—å„ªåŒ–**
   - æäº¤è©³æƒ…å¿«å–ï¼ˆ2åˆ†é˜ TTLï¼‰
   - æ’è¡Œæ¦œå¿«å–ï¼ˆ5åˆ†é˜ TTLï¼‰

2. **æ²¿ç”¨èˆŠ NOJ**
   - ç”¨æˆ¶é¡Œç›®é«˜åˆ†å¿«å–ï¼ˆ10åˆ†é˜ TTLï¼‰
   - æäº¤æ¬Šé™å¿«å–ï¼ˆ1åˆ†é˜ TTLï¼‰
   - é©—è­‰ Token æ©Ÿåˆ¶

#### ç¬¬ä¸‰éšæ®µï¼ˆç›£æ§å®Œå–„ï¼‰
1. **ç›£æ§ç³»çµ±**
   - `CacheHitRateMonitor`ï¼ˆå‘½ä¸­ç‡ç›£æ§ï¼‰
   - `RedisMemoryMonitor`ï¼ˆè¨˜æ†¶é«”ç›£æ§ï¼‰
   - Management Commands (`cache_stats`, `monitor_redis_memory`)

2. **è­¦å ±èˆ‡å„ªåŒ–**
   - ä½å‘½ä¸­ç‡è­¦å ±ï¼ˆ< 70%ï¼‰
   - è¨˜æ†¶é«”è­¦å ±ï¼ˆ> 80%ï¼‰
   - æ•ˆèƒ½èª¿å„ªèˆ‡å£“ç¸®

### ä¸‰å¤§é—œéµå•é¡Œè§£æ±ºæ–¹æ¡ˆç¸½çµ

| å•é¡Œ | è§£æ±ºæ–¹æ¡ˆ | å¯¦ä½œè¤‡é›œåº¦ | å¿…é ˆå¯¦ä½œ |
|------|---------|----------|---------|
| **1. ç·©å­˜èˆ‡è³‡æ–™åº«ä¸ä¸€è‡´** | Django signals ä¸»å‹•æ¸…é™¤ + TTL å…œåº• | ç°¡å–® | æ˜¯ |
| **2. Redis æŸ¥è©¢é˜»å¡** | è¶…æ™‚é™ç´šï¼ˆ0.5ç§’ï¼‰+ åˆ†æ•£å¼é–è¶…æ™‚ï¼ˆ3ç§’ï¼‰ | ä¸­ç­‰ | æ˜¯ |
| **3. å…§å­˜çˆ†æ»¿** | `maxmemory=2gb` + LRU æ·˜æ±° + ç›£æ§è­¦å ± | ç°¡å–® | æ˜¯ |

**é—œéµè¨­è¨ˆåŸå‰‡**ï¼š
1. **å¯§å¯æ…¢ï¼Œä¸å¯éŒ¯**ï¼šRedis æ•…éšœæ™‚é™ç´šåˆ°è³‡æ–™åº«ï¼Œä¿è­‰æœå‹™å¯ç”¨
2. **TTL æ˜¯ä¿éšª**ï¼šå³ä½¿ signals å¤±æ•ˆï¼Œå¿«å–æœ€å¤šå»¶é² 5 åˆ†é˜
3. **ç›£æ§æ˜¯çœ¼ç›**ï¼šå¿«å–å‘½ä¸­ç‡å’Œè¨˜æ†¶é«”ä½¿ç”¨å¿…é ˆå¯è§€æ¸¬
4. **è¶…æ™‚æ˜¯åº•ç·š**ï¼šæ‰€æœ‰ Redis æ“ä½œéƒ½æœ‰è¶…æ™‚ï¼Œä¸èƒ½ç„¡é™ç­‰å¾…

### å¯¦ä½œå»ºè­°

1. **ä¿å®ˆå„ªå…ˆ**ï¼šåªå¿«å–æ¥µé«˜é »å’Œå°é«”é©—é—œéµçš„è³‡æ–™
2. **å®‰å…¨ç¬¬ä¸€**ï¼šå¿…é ˆå¯¦ä½œå¸ƒéš†éæ¿¾å™¨ã€åˆ†æ•£å¼é–ã€è¶…æ™‚é™ç´š
3. **åƒè€ƒèˆŠ NOJ**ï¼šæ²¿ç”¨å·²é©—è­‰çš„ TTL æ™‚é–“
4. **ç°¡åŒ–å¤±æ•ˆ**ï¼šä¸»è¦ä¾è³´ TTL è‡ªå‹•éæœŸ
5. **ç›£æ§å¿…å‚™**ï¼šè¿½è¹¤ cache hit rate å’Œè¨˜æ†¶é«”ä½¿ç”¨

---

**æ–‡ä»¶ç‰ˆæœ¬**: v2.0ï¼ˆä¿å®ˆç­–ç•¥ + å®Œæ•´å®¹éŒ¯ï¼‰  
**æœ€å¾Œæ›´æ–°**: 2025å¹´11æœˆ2æ—¥  
**ç¶­è­·è€…**: Backend Team